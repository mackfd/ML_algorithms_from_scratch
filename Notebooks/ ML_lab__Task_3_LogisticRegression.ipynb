{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62da667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc68ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b387649f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = [5, 9]\n",
    "cov = [[2.5, 0.8], [0.8, 0.5]]\n",
    "X_p = np.random.multivariate_normal(mean, cov, n_samples).T\n",
    "X_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1dbb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = [11, 3]\n",
    "cov = [[3, -1.3], [-1.3, 1.2]]\n",
    "X_n_1 = np.random.multivariate_normal([11, 3], cov, int(n_samples/2)).T\n",
    "X_n_2 = np.random.multivariate_normal([5, 2], cov, n_samples-int(n_samples/2)).T\n",
    "X_n = np.hstack([X_n_1, X_n_2])\n",
    "X_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f62ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_p = np.vstack([X_p, np.ones_like(X_p[0])])\n",
    "XY_n = np.vstack([X_n, np.zeros_like(X_n[0])])\n",
    "XY_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781f5bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY = np.hstack([XY_n, XY_p])\n",
    "data_XY = np.copy(XY).T\n",
    "np.random.shuffle(data_XY)\n",
    "data_train = data_XY[:1600]\n",
    "data_test = data_XY[:400]\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416bb246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params_initial = np.array([[-3], [3]])\n",
    "params_initial = np.array([[1], [1]])\n",
    "params_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a07eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_regression_grad():\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _compute_loss(self, y, h, epsilon: float = 1e-5):\n",
    "         \n",
    "        # calculate binary cross entropy as loss \n",
    "        loss = (1/self.batch_size)*(((-y).T @ np.log(h + epsilon))-((1-y).T @ np.log(1-h + epsilon)))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, x, y, epochs, batch_size, lr):\n",
    "        row, col = x.shape\n",
    "        \n",
    "        # Initializing weights and bias\n",
    "        self.w = np.zeros((col,1))\n",
    "        self.w0 = 1\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # defining bath size\n",
    "        num_batches = x.shape[0]//self.batch_size\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch: \", epoch)\n",
    "            for batch_num in range(num_batches+1):\n",
    "                \n",
    "                # slicing batch data\n",
    "                start = batch_num * self.batch_size\n",
    "                end = (batch_num + 1) * self.batch_size\n",
    "                \n",
    "                x_batched = x[start:end]\n",
    "                y_batched = np.array(y[start:end]).reshape((-1, 1))\n",
    "                \n",
    "                # predict for the epoch and batch\n",
    "                # at first iteration we are using initial w/theta\n",
    "                y_hat = self._sigmoid(np.dot(x_batched, self.w) + self.w0)\n",
    "                \n",
    "                # calculate gradient for weigths/theta\n",
    "                error = y_hat - y_batched\n",
    "        \n",
    "                gradient_w = (1/self.batch_size)*np.dot(x_batched.T, error)\n",
    "                gradient_w0 = (1/self.batch_size)*np.sum(error) \n",
    "                \n",
    "                # adjusting weights/theta with learning rate annd calculated gradient \n",
    "                self.w -= self.lr*gradient_w\n",
    "                self.w0 -= self.lr*gradient_w0\n",
    "                \n",
    "            # loss compute per epoch\n",
    "            loss = self._compute_loss(y, self._sigmoid(np.dot(x, self.w) + self.w0))\n",
    "            print(\"loss: \",loss)\n",
    "        \n",
    "        return self.w, self.w0        \n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        \n",
    "        # predict on text data with calculated weigths/theta\n",
    "        predictions = self._sigmoid(np.dot(x_test, self.w) + self.w0)\n",
    "        \n",
    "        # rounding up values to get classes\n",
    "        predictions = np.round(predictions)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25219b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train[:,0:2]\n",
    "y_train = data_train[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae4c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Logistic_regression_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ee532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss:  [7.29865126]\n",
      "epoch:  1\n",
      "loss:  [5.5776989]\n",
      "epoch:  2\n",
      "loss:  [4.71298937]\n",
      "epoch:  3\n",
      "loss:  [4.19097032]\n",
      "epoch:  4\n",
      "loss:  [3.83881459]\n",
      "epoch:  5\n",
      "loss:  [3.58330204]\n",
      "epoch:  6\n",
      "loss:  [3.38821848]\n",
      "epoch:  7\n",
      "loss:  [3.23358555]\n",
      "epoch:  8\n",
      "loss:  [3.10745648]\n",
      "epoch:  9\n",
      "loss:  [3.00223211]\n",
      "epoch:  10\n",
      "loss:  [2.91283711]\n",
      "epoch:  11\n",
      "loss:  [2.8357454]\n",
      "epoch:  12\n",
      "loss:  [2.7684262]\n",
      "epoch:  13\n",
      "loss:  [2.70901281]\n",
      "epoch:  14\n",
      "loss:  [2.65609602]\n",
      "epoch:  15\n",
      "loss:  [2.60859054]\n",
      "epoch:  16\n",
      "loss:  [2.56564589]\n",
      "epoch:  17\n",
      "loss:  [2.52658539]\n",
      "epoch:  18\n",
      "loss:  [2.49086332]\n",
      "epoch:  19\n",
      "loss:  [2.4580343]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.73759714],\n",
       "        [ 0.66128902]]),\n",
       " 0.8239512026802385)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(x_train,y_train,20,100,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7abb3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_XY[:400]\n",
    "x_test = data_test[:,0:2]\n",
    "y_test = data_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce305dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2b7713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 382.0\n",
      "Incorrect: 18.0\n",
      "Accuracy: 95.50%\n"
     ]
    }
   ],
   "source": [
    "incorrect = sum((-1)*(y_test - y_hat.flatten()))\n",
    "correct = len(y_test) - incorrect\n",
    "print(\"Correct: {}\".format(correct))\n",
    "print(\"Incorrect: {}\".format(incorrect))\n",
    "print(\"Accuracy: {:2.2%}\".format(correct/len(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
